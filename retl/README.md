# retl: Reversed ETL Module

## Overview

The `retl` module is a Reversed ETL (Extract, Transform, Load) pipeline written in Python. Its primary purpose is to extract data from a data warehouse (currently Google BigQuery) and update the transactional database used by the API module (currently SQLite). This ensures that the API can serve the most up-to-date data generated by upstream processes.

## Key Features

- **Source:** Extracts data from Google BigQuery.
- **Destination:** Loads data into a SQLite database used by the API.
- **No Transformations:** Data is transferred as-is, assuming that the `dbt_ref_*` tables in BigQuery are already correct and up-to-date.
- **Extensible Design:** The architecture is designed to support additional data sources, destinations, and transformation steps in the future.

## Workflow

1. **Extract:** Pulls data from the relevant `dbt_ref_*` tables in BigQuery.
2. **Load:** Writes the extracted data directly into the SQLite database.

## Usage

- Ensure that the `dbt_ref_*` tables in BigQuery are up-to-date before running the pipeline.
- Run the ETL process to sync data from BigQuery to SQLite.
- The API will automatically use the updated SQLite database to serve requests.

## Extensibility

The module is structured to allow easy addition of:
- New data sources (e.g., other warehouses)
- New destinations (e.g., different transactional databases)
- Transformation steps between extraction and loading

## Requirements

- Python (see `requirements.txt` for dependencies)
- Access to Google BigQuery
- Access to the SQLite database used by the API

## Directory Structure

- `app/` - Main application code for the ETL process
- `tests/` - Test suite for the ETL logic
- `requirements.txt` - Python dependencies
- `Dockerfile` - Containerization support

## License

See repository root for license information.
